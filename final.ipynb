{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4872fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "665c0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainCfg:\n",
    "    # You can swap this to a larger model if your GPU allows:\n",
    "    # e.g. 'microsoft/deberta-v3-large'\n",
    "    # If you want to move closer to the 9th-place approach,\n",
    "    # you could later switch to a Gemma model:\n",
    "    # 'google/gemma-2-2b-it' (or 9b on Kaggle with 4-bit).\n",
    "    model_name: str = \"roberta-base\"\n",
    "\n",
    "    max_length: int = 512\n",
    "    train_batch_size: int = 4\n",
    "    valid_batch_size: int = 4\n",
    "    learning_rate: float = 2e-5\n",
    "    epochs: int = 3\n",
    "    seed: int = 42\n",
    "\n",
    "    # Paths (Kaggle-compatible structure)\n",
    "    train_path: str = \"data/train.csv\"\n",
    "    test_path: str = \"data/test.csv\"\n",
    "    sub_path: str = \"data/ours_submission.csv\"\n",
    "\n",
    "    device: torch.device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "cfg = TrainCfg()\n",
    "set_seed(cfg.seed)\n",
    "print(\"Using device:\", cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "79950df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import ast\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Robustly convert the CSV field to plain text.\n",
    "    Handles:\n",
    "      - None / NaN\n",
    "      - actual Python lists\n",
    "      - stringified lists like '[\"hello\", \"world\"]'\n",
    "      - malformed '[...]' that can't be parsed (falls back gracefully)\n",
    "    \"\"\"\n",
    "    # 1. Missing values\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        return \"\"\n",
    "\n",
    "    # 2. Already a list -> join its elements\n",
    "    if isinstance(text, list):\n",
    "        return \" \".join(\"\" if t is None else str(t) for t in text)\n",
    "\n",
    "    # 3. Convert anything else to str\n",
    "    s = str(text)\n",
    "\n",
    "    # 4. Try to parse stringified list safely\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            # Not a valid Python literal, just strip the brackets and treat as text\n",
    "            return s.strip(\"[]\")\n",
    "\n",
    "        # If it parsed successfully, handle lists or other literals\n",
    "        if isinstance(parsed, list):\n",
    "            return \" \".join(\"\" if t is None else str(t) for t in parsed)\n",
    "        else:\n",
    "            # e.g. parsed into a single string or number\n",
    "            return str(parsed)\n",
    "\n",
    "    # 5. Plain string\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "class LMSYSDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Convert to records for speed\n",
    "        self.data = self.df[[\"prompt\", \"response_a\", \"response_b\"]].to_dict(\"records\")\n",
    "\n",
    "        # Pre-compute labels\n",
    "        if not self.is_test:\n",
    "            self.labels = []\n",
    "            for _, row in self.df.iterrows():\n",
    "                if row[\"winner_model_a\"] == 1:\n",
    "                    label = 0\n",
    "                elif row[\"winner_model_b\"] == 1:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 2\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        \n",
    "        # Safe access to separator token (fallback to space if None)\n",
    "        sep = self.tokenizer.sep_token if self.tokenizer.sep_token is not None else \" \"\n",
    "        \n",
    "        # FORCE conversion to standard Python string to avoid TypeErrors\n",
    "        # This fixes the crash on rows with weird types (e.g. numpy str_, nan, etc.)\n",
    "        p = str(row.get(\"prompt\", \"\"))\n",
    "        ra = str(row.get(\"response_a\", \"\"))\n",
    "        rb = str(row.get(\"response_b\", \"\"))\n",
    "        \n",
    "        text = f\"{p} {sep} {ra} {sep} {rb}\"\n",
    "\n",
    "        # Tokenize\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback for extremely weird edge cases: treat as empty string\n",
    "            print(f\"Warning: Tokenization failed at idx {idx}. Using empty text.\")\n",
    "            print(f\"Offending text: prompt={p}, response_a={ra}, response_b={rb}\")\n",
    "            inputs = self.tokenizer(\n",
    "                \"\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        if not self.is_test:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    num_classes = probs.shape[1]\n",
    "    labels = np.array(labels, dtype=int)\n",
    "    labels_oh = np.eye(num_classes)[labels]\n",
    "\n",
    "    return {\"log_loss\": log_loss(labels_oh, probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf85baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original train shape: (57477, 9)\n",
      "Original test shape: (3, 4)\n",
      "train: Dropping 26 empty rows\n",
      "After cleaning, train shape: (57451, 9)\n",
      "After cleaning, test shape: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def strip_surrogates(text: str) -> str:\n",
    "    \"\"\"Remove characters that cannot be encoded in UTF-8 (surrogates etc).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return text.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "\n",
    "def strip_control_chars(s: str) -> str:\n",
    "    \"\"\"Remove ASCII control chars except \\n and \\t.\"\"\"\n",
    "    return re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \"\", s)\n",
    "\n",
    "def safe_literal_list(text: str):\n",
    "    \"\"\"Safely parse stringified list like '[\"a\", \"b\"]', else return None.\"\"\"\n",
    "    if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(text)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def clean_text(x):\n",
    "    \"\"\"Complete cleaning pipeline.\"\"\"\n",
    "    # None / NaN\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return \"\"\n",
    "\n",
    "    # Already a Python list\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join(strip_surrogates(strip_control_chars(str(t))) for t in x)\n",
    "\n",
    "    # Convert to string\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Try parsing JSON-like list\n",
    "    parsed_list = safe_literal_list(s)\n",
    "    if parsed_list is not None:\n",
    "        # Join list elements\n",
    "        return \" \".join(strip_surrogates(strip_control_chars(str(t))) for t in parsed_list)\n",
    "\n",
    "    # Strip unicode surrogates and control characters\n",
    "    s = strip_surrogates(s)\n",
    "    s = strip_control_chars(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "train_df = pd.read_csv(cfg.train_path)\n",
    "test_df = pd.read_csv(cfg.test_path)\n",
    "\n",
    "print(\"Original train shape:\", train_df.shape)\n",
    "print(\"Original test shape:\", test_df.shape)\n",
    "\n",
    "for df_name, df in [(\"train\", train_df), (\"test\", test_df)]:\n",
    "    # 1) Drop NaN rows\n",
    "    df.dropna(subset=[\"prompt\", \"response_a\", \"response_b\"], inplace=True)\n",
    "\n",
    "    # 2) Clean text fully\n",
    "    for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "        df[col] = df[col].apply(clean_text)\n",
    "\n",
    "    # 3) Drop empty/whitespace rows\n",
    "    mask_empty = (\n",
    "        df[\"prompt\"].str.strip().eq(\"\") |\n",
    "        df[\"response_a\"].str.strip().eq(\"\") |\n",
    "        df[\"response_b\"].str.strip().eq(\"\")\n",
    "    )\n",
    "    to_drop = mask_empty.sum()\n",
    "    if to_drop > 0:\n",
    "        print(f\"{df_name}: Dropping {to_drop} empty rows\")\n",
    "        df.drop(df[mask_empty].index, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"After cleaning, {df_name} shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9b4fbe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model...\n",
      "Train samples: 51705, Val samples: 5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28416\\628969652.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38781' max='38781' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38781/38781 1:29:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.095900</td>\n",
       "      <td>1.097005</td>\n",
       "      <td>1.097114</td>\n",
       "      <td>37.673000</td>\n",
       "      <td>152.523000</td>\n",
       "      <td>38.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.094900</td>\n",
       "      <td>1.096594</td>\n",
       "      <td>1.096550</td>\n",
       "      <td>55.778400</td>\n",
       "      <td>103.015000</td>\n",
       "      <td>25.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.099400</td>\n",
       "      <td>1.097170</td>\n",
       "      <td>1.097041</td>\n",
       "      <td>54.686000</td>\n",
       "      <td>105.073000</td>\n",
       "      <td>26.277000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer & model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "# Ensure pad_token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Split data\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=cfg.seed,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize Datasets (Now fast because we don't tokenize yet)\n",
    "train_dataset = LMSYSDataset(train_split, tokenizer, cfg.max_length, is_test=False)\n",
    "valid_dataset = LMSYSDataset(val_split, tokenizer, cfg.max_length, is_test=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(valid_dataset)}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Use DataCollator to pad batches dynamically to the longest sequence in the BATCH\n",
    "# (instead of padding everything to max_length=1024, which wastes GPU memory)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./outputs/{cfg.model_name.replace('/', '_')}_finetuned\",\n",
    "    num_train_epochs=cfg.epochs,\n",
    "    per_device_train_batch_size=cfg.train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.valid_batch_size,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, # Recommended for T4/P100 GPUs (Kaggle) to save memory/speed up\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # <--- Added this\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20b03b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainCfg(\n",
    "    model_name=\"distilroberta-base\", \n",
    "    epochs=20,\n",
    "    learning_rate=3e-5, \n",
    "    train_batch_size=64, \n",
    "    valid_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e5ea0c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model...\n",
      "Train samples: 51705, Val samples: 5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28416\\3441798207.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16160' max='16160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16160/16160 2:44:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.084200</td>\n",
       "      <td>1.077140</td>\n",
       "      <td>1.077139</td>\n",
       "      <td>20.694200</td>\n",
       "      <td>277.663000</td>\n",
       "      <td>4.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.075300</td>\n",
       "      <td>1.092544</td>\n",
       "      <td>1.092539</td>\n",
       "      <td>20.744000</td>\n",
       "      <td>276.996000</td>\n",
       "      <td>4.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.044400</td>\n",
       "      <td>1.089451</td>\n",
       "      <td>1.089446</td>\n",
       "      <td>20.872800</td>\n",
       "      <td>275.286000</td>\n",
       "      <td>4.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>1.113672</td>\n",
       "      <td>1.113669</td>\n",
       "      <td>20.838100</td>\n",
       "      <td>275.745000</td>\n",
       "      <td>4.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>1.236092</td>\n",
       "      <td>1.236091</td>\n",
       "      <td>20.459400</td>\n",
       "      <td>280.848000</td>\n",
       "      <td>4.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>1.368358</td>\n",
       "      <td>1.368353</td>\n",
       "      <td>20.805100</td>\n",
       "      <td>276.183000</td>\n",
       "      <td>4.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>1.448684</td>\n",
       "      <td>1.448684</td>\n",
       "      <td>20.817200</td>\n",
       "      <td>276.022000</td>\n",
       "      <td>4.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>1.850870</td>\n",
       "      <td>1.850864</td>\n",
       "      <td>21.085200</td>\n",
       "      <td>272.514000</td>\n",
       "      <td>4.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>2.009864</td>\n",
       "      <td>2.009868</td>\n",
       "      <td>20.692700</td>\n",
       "      <td>277.682000</td>\n",
       "      <td>4.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>2.180743</td>\n",
       "      <td>2.180742</td>\n",
       "      <td>20.479900</td>\n",
       "      <td>280.568000</td>\n",
       "      <td>4.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>2.305615</td>\n",
       "      <td>2.305612</td>\n",
       "      <td>20.684500</td>\n",
       "      <td>277.793000</td>\n",
       "      <td>4.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>2.570845</td>\n",
       "      <td>2.570812</td>\n",
       "      <td>20.800500</td>\n",
       "      <td>276.244000</td>\n",
       "      <td>4.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>2.835031</td>\n",
       "      <td>2.835025</td>\n",
       "      <td>20.656200</td>\n",
       "      <td>278.173000</td>\n",
       "      <td>4.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>2.868795</td>\n",
       "      <td>2.868775</td>\n",
       "      <td>20.832100</td>\n",
       "      <td>275.824000</td>\n",
       "      <td>4.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>3.121780</td>\n",
       "      <td>3.121744</td>\n",
       "      <td>20.540000</td>\n",
       "      <td>279.747000</td>\n",
       "      <td>4.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>3.149167</td>\n",
       "      <td>3.149107</td>\n",
       "      <td>20.902400</td>\n",
       "      <td>274.897000</td>\n",
       "      <td>4.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>3.434790</td>\n",
       "      <td>3.434703</td>\n",
       "      <td>20.912100</td>\n",
       "      <td>274.769000</td>\n",
       "      <td>4.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>3.552703</td>\n",
       "      <td>3.552603</td>\n",
       "      <td>20.729600</td>\n",
       "      <td>277.188000</td>\n",
       "      <td>4.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>3.722059</td>\n",
       "      <td>3.721927</td>\n",
       "      <td>20.537400</td>\n",
       "      <td>279.782000</td>\n",
       "      <td>4.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>3.800005</td>\n",
       "      <td>3.799871</td>\n",
       "      <td>20.757600</td>\n",
       "      <td>276.814000</td>\n",
       "      <td>4.336000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer & model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "# Ensure pad_token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Split data\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=cfg.seed,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize Datasets (Now fast because we don't tokenize yet)\n",
    "train_dataset = LMSYSDataset(train_split, tokenizer, cfg.max_length, is_test=False)\n",
    "valid_dataset = LMSYSDataset(val_split, tokenizer, cfg.max_length, is_test=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(valid_dataset)}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Use DataCollator to pad batches dynamically to the longest sequence in the BATCH\n",
    "# (instead of padding everything to max_length=1024, which wastes GPU memory)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./outputs/{cfg.model_name.replace('/', '_')}_finetuned\",\n",
    "    num_train_epochs=cfg.epochs,\n",
    "    per_device_train_batch_size=cfg.train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.valid_batch_size,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, # Recommended for T4/P100 GPUs (Kaggle) to save memory/speed up\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # <--- Added this\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainCfg(\n",
    "    model_name=\"distilroberta-base\", \n",
    "    \n",
    "    learning_rate=3e-5, \n",
    "    train_batch_size=16, \n",
    "    valid_batch_size=16\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e6c3c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference with A/B swap TTA...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: data/ours_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.347363</td>\n",
       "      <td>0.347363</td>\n",
       "      <td>0.305274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.347366</td>\n",
       "      <td>0.347366</td>\n",
       "      <td>0.305269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.347362</td>\n",
       "      <td>0.347362</td>\n",
       "      <td>0.305276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.347363        0.347363    0.305274\n",
       "1   211333        0.347366        0.347366    0.305269\n",
       "2  1233961        0.347362        0.347362    0.305276"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting inference with A/B swap TTA...\")\n",
    "\n",
    "# 1. Normal test dataset: (prompt, A, B)\n",
    "test_ds_normal = LMSYSDataset(test_df, tokenizer, cfg.max_length, is_test=True)\n",
    "\n",
    "# 2. Swapped test dataset: (prompt, B, A)\n",
    "test_df_swapped = test_df.copy()\n",
    "test_df_swapped[[\"response_a\", \"response_b\"]] = test_df_swapped[[\"response_b\", \"response_a\"]]\n",
    "test_ds_swapped = LMSYSDataset(test_df_swapped, tokenizer, cfg.max_length, is_test=True)\n",
    "\n",
    "# Predict\n",
    "preds_normal = trainer.predict(test_ds_normal).predictions\n",
    "preds_swapped = trainer.predict(test_ds_swapped).predictions\n",
    "\n",
    "probs_normal = F.softmax(torch.tensor(preds_normal), dim=-1).numpy()\n",
    "probs_swapped = F.softmax(torch.tensor(preds_swapped), dim=-1).numpy()\n",
    "\n",
    "# For swapped, model's \"class 0\" means \"first response wins\" which is B in original,\n",
    "# and \"class 1\" means \"second response wins\" which is A in original.\n",
    "# So we need to swap back the first two probability columns.\n",
    "probs_swapped_fixed = np.zeros_like(probs_swapped)\n",
    "probs_swapped_fixed[:, 0] = probs_swapped[:, 1]  # B->A\n",
    "probs_swapped_fixed[:, 1] = probs_swapped[:, 0]  # A->B\n",
    "probs_swapped_fixed[:, 2] = probs_swapped[:, 2]  # tie unchanged\n",
    "\n",
    "# Ensemble\n",
    "final_probs = (probs_normal + probs_swapped_fixed) / 2.0\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test_df[\"id\"].values,\n",
    "        \"winner_model_a\": final_probs[:, 0],\n",
    "        \"winner_model_b\": final_probs[:, 1],\n",
    "        \"winner_tie\": final_probs[:, 2],\n",
    "    }\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.dirname(cfg.sub_path), exist_ok=True)\n",
    "submission.to_csv(cfg.sub_path, index=False)\n",
    "print(\"Saved submission to:\", cfg.sub_path)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
