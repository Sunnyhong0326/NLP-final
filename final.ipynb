{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4872fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\NLP_final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainCfg:\n",
    "    # Use 'microsoft/deberta-v3-large' for better results, 'small' for speed/debugging\n",
    "    model_name = 'microsoft/deberta-v3-small' \n",
    "    max_length = 1024\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 8\n",
    "    learning_rate = 2e-5\n",
    "    epochs = 10\n",
    "    n_folds = 5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed = 42\n",
    "    \n",
    "    train_path = 'data/train.csv'\n",
    "    test_path = 'data/test.csv'\n",
    "    sub_path = 'data/ours_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79950df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# DATA PREPROCESSING\n",
    "# =========================================================================================\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    The dataset sometimes stores conversation history as a string representation \n",
    "    of a list (e.g., \"['hello', 'hi']\"). We need to join them into plain text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If it looks like a list, try to evaluate it\n",
    "        if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "            import ast\n",
    "            text_list = ast.literal_eval(text)\n",
    "            return \" \".join(text_list)\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def prepare_input(cfg, text, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=cfg.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=None, # Return standard lists, not tensors yet\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "class LMSYSDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Extract and Clean Text\n",
    "        prompt = process_text(row['prompt'])\n",
    "        resp_a = process_text(row['response_a'])\n",
    "        resp_b = process_text(row['response_b'])\n",
    "        \n",
    "        # 2. Construct Input String\n",
    "        # Structure: [CLS] Prompt [SEP] Response A [SEP] Response B [SEP]\n",
    "        # Note: DeBERTa v3 uses [SEP] to separate segments naturally\n",
    "        text = f\"{prompt} [SEP] {resp_a} [SEP] {resp_b}\"\n",
    "        \n",
    "        # 3. Tokenize\n",
    "        inputs = prepare_input(TrainCfg, text, self.tokenizer)\n",
    "        \n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        # 4. Handle Labels (Only for Training)\n",
    "        if not self.is_test:\n",
    "            # Create a probability distribution target\n",
    "            # [A wins, B wins, Tie]\n",
    "            label_vec = np.zeros(3, dtype=np.float32)\n",
    "            if row['winner_model_a'] == 1:\n",
    "                label_vec[0] = 1.0\n",
    "            elif row['winner_model_b'] == 1:\n",
    "                label_vec[1] = 1.0\n",
    "            else:\n",
    "                label_vec[2] = 1.0\n",
    "                \n",
    "            item[\"labels\"] = torch.tensor(label_vec, dtype=torch.float)\n",
    "            \n",
    "        return item\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    score = log_loss(labels, probs)\n",
    "    return {\"log_loss\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Using Device: {TrainCfg.device}\")\n",
    "\n",
    "# 1. Load Data\n",
    "# In a real Kaggle run, use the paths in CFG.\n",
    "# For local testing, we create a dummy dataframe.\n",
    "if os.path.exists(TrainCfg.train_path):\n",
    "    train_df = pd.read_csv(TrainCfg.train_path)\n",
    "    test_df = pd.read_csv(TrainCfg.test_path)\n",
    "else:\n",
    "    raise ValueError(\"No data found\")\n",
    "\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "\n",
    "# 2. Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TrainCfg.model_name)\n",
    "\n",
    "# 3. Train-Validation Split\n",
    "# We use a simple split for the starter. \n",
    "# For a winning solution, loop over k-folds.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_split, val_split = train_test_split(train_df, test_size=0.1, random_state=TrainCfg.seed)\n",
    "\n",
    "train_dataset = LMSYSDataset(train_split, tokenizer, TrainCfg.max_length)\n",
    "valid_dataset = LMSYSDataset(val_split, tokenizer, TrainCfg.max_length)\n",
    "\n",
    "# 4. Model Setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TrainCfg.model_name,\n",
    "    num_labels=3,\n",
    "    problem_type=\"multi_label_classification\" \n",
    "    # We treat it as multi-label so we can use float targets (BCEWithLogitsLoss logic internal)\n",
    "    # However, custom loss is often better. HF default for float labels is BCEWithLogitsLoss.\n",
    ")\n",
    "\n",
    "# 5. Training Arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"output_lmsys\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=TrainCfg.learning_rate,\n",
    "    per_device_train_batch_size=TrainCfg.train_batch_size,\n",
    "    per_device_eval_batch_size=TrainCfg.valid_batch_size,\n",
    "    num_train_epochs=TrainCfg.epochs,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "print(\"Starting Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Inference with TTA...\")\n",
    "\n",
    "# Strategy:\n",
    "# Prediction 1: Prompt + RespA + RespB\n",
    "# Prediction 2: Prompt + RespB + RespA (Swap)\n",
    "# Result = (Pred1 + Swap_Probabilities(Pred2)) / 2\n",
    "\n",
    "# Prepare Test Datasets\n",
    "# 1. Normal\n",
    "test_ds_normal = LMSYSDataset(test_df, tokenizer, TrainCfg.max_length, is_test=True)\n",
    "\n",
    "# 2. Swapped (Swap columns in a copy of the dataframe)\n",
    "test_df_swapped = test_df.copy()\n",
    "test_df_swapped.rename(columns={'response_a': 'response_b', 'response_b': 'response_a'}, inplace=True)\n",
    "test_ds_swapped = LMSYSDataset(test_df_swapped, tokenizer, TrainCfg.max_length, is_test=True)\n",
    "\n",
    "# Predict\n",
    "preds_normal = trainer.predict(test_ds_normal).predictions\n",
    "preds_swapped = trainer.predict(test_ds_swapped).predictions\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "probs_normal = torch.nn.functional.softmax(torch.tensor(preds_normal), dim=-1).numpy()\n",
    "probs_swapped = torch.nn.functional.softmax(torch.tensor(preds_swapped), dim=-1).numpy()\n",
    "\n",
    "# Swap the probabilities of A and B back for the swapped prediction\n",
    "# If the model said B wins (index 1) in the swapped version, that actually means A wins (index 0)\n",
    "# Indices: 0=A, 1=B, 2=Tie\n",
    "probs_swapped_fixed = np.zeros_like(probs_swapped)\n",
    "probs_swapped_fixed[:, 0] = probs_swapped[:, 1] # B becomes A\n",
    "probs_swapped_fixed[:, 1] = probs_swapped[:, 0] # A becomes B\n",
    "probs_swapped_fixed[:, 2] = probs_swapped[:, 2] # Tie stays Tie\n",
    "\n",
    "# Ensemble\n",
    "final_probs = (probs_normal + probs_swapped_fixed) / 2\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': final_probs[:, 0],\n",
    "    'winner_model_b': final_probs[:, 1],\n",
    "    'winner_tie': final_probs[:, 2]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d010be",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(TrainCfg.sub_path, index=False)\n",
    "print(\"Submission saved successfully!\")\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
