{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3514dd6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:03.335177Z",
     "iopub.status.busy": "2025-11-30T21:02:03.334964Z",
     "iopub.status.idle": "2025-11-30T21:02:04.794082Z",
     "shell.execute_reply": "2025-11-30T21:02:04.793141Z"
    },
    "papermill": {
     "duration": 1.464462,
     "end_time": "2025-11-30T21:02:04.795961",
     "exception": false,
     "start_time": "2025-11-30T21:02:03.331499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/config.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/merges.txt\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/trainer_state.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/training_args.bin\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/tokenizer.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/vocab.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/tokenizer_config.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/scaler.pt\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/scheduler.pt\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/model.safetensors\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/special_tokens_map.json\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/optimizer.pt\n",
      "/kaggle/input/distillroberta/distilroberta-base_finetuned/checkpoint-6464/rng_state.pth\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/config.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/merges.txt\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/trainer_state.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/training_args.bin\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/tokenizer.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/vocab.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/tokenizer_config.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/scaler.pt\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/scheduler.pt\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/model.safetensors\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/special_tokens_map.json\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/optimizer.pt\n",
      "/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464/rng_state.pth\n",
      "/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\n",
      "/kaggle/input/lmsys-chatbot-arena/train.csv\n",
      "/kaggle/input/lmsys-chatbot-arena/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eeba79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:04.804405Z",
     "iopub.status.busy": "2025-11-30T21:02:04.804041Z",
     "iopub.status.idle": "2025-11-30T21:02:04.913520Z",
     "shell.execute_reply": "2025-11-30T21:02:04.912638Z"
    },
    "papermill": {
     "duration": 0.115676,
     "end_time": "2025-11-30T21:02:04.915141",
     "exception": false,
     "start_time": "2025-11-30T21:02:04.799465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[protobuf shim ready] protobuf=6.33.0, MessageFactory.has(GetPrototype=False, GetMessageClass=False)\n"
     ]
    }
   ],
   "source": [
    "# --- Protobuf compatibility shim (must run BEFORE any other imports) ---\n",
    "import os\n",
    "os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")  # safer in Kaggle\n",
    "\n",
    "from google.protobuf import __version__ as _pb_ver\n",
    "from google.protobuf import message_factory as _mf\n",
    "from google.protobuf import symbol_database as _symdb\n",
    "\n",
    "# Make both names available regardless of protobuf version.\n",
    "# Some libs call GetPrototype (3.x), others call GetMessageClass (4.x).\n",
    "\n",
    "# Patch MessageFactory\n",
    "if not hasattr(_mf.MessageFactory, \"GetMessageClass\") and hasattr(_mf.MessageFactory, \"GetPrototype\"):\n",
    "    # Env has only GetPrototype -> define GetMessageClass in terms of it\n",
    "    def _GetMessageClass(self, descriptor):\n",
    "        return self.GetPrototype(descriptor)\n",
    "    _mf.MessageFactory.GetMessageClass = _GetMessageClass\n",
    "\n",
    "if not hasattr(_mf.MessageFactory, \"GetPrototype\") and hasattr(_mf.MessageFactory, \"GetMessageClass\"):\n",
    "    # Env has only GetMessageClass -> define GetPrototype in terms of it\n",
    "    def _GetPrototype(self, descriptor):\n",
    "        return self.GetMessageClass(descriptor)\n",
    "    _mf.MessageFactory.GetPrototype = _GetPrototype\n",
    "\n",
    "# Patch SymbolDatabase\n",
    "_sym = _symdb.Default()\n",
    "if not hasattr(_sym, \"GetMessageClass\") and hasattr(_sym, \"GetPrototype\"):\n",
    "    def _sym_GetMessageClass(descriptor):\n",
    "        return _sym.GetPrototype(descriptor)\n",
    "    _sym.GetMessageClass = _sym_GetMessageClass\n",
    "\n",
    "if not hasattr(_sym, \"GetPrototype\") and hasattr(_sym, \"GetMessageClass\"):\n",
    "    def _sym_GetPrototype(descriptor):\n",
    "        return _sym.GetMessageClass(descriptor)\n",
    "    _sym.GetPrototype = _sym_GetPrototype\n",
    "\n",
    "print(f\"[protobuf shim ready] protobuf={_pb_ver}, \"\n",
    "      f\"MessageFactory.has(GetPrototype={hasattr(_mf.MessageFactory,'GetPrototype')}, \"\n",
    "      f\"GetMessageClass={hasattr(_mf.MessageFactory,'GetMessageClass')})\")\n",
    "# --- end shim ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7aeb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:04.923646Z",
     "iopub.status.busy": "2025-11-30T21:02:04.923263Z",
     "iopub.status.idle": "2025-11-30T21:02:17.278699Z",
     "shell.execute_reply": "2025-11-30T21:02:17.278104Z"
    },
    "papermill": {
     "duration": 12.36104,
     "end_time": "2025-11-30T21:02:17.280137",
     "exception": false,
     "start_time": "2025-11-30T21:02:04.919097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re, ast, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# --- Paths (edit MODEL_DIR to your dataset path) ---\n",
    "COMP_DIR  = \"/kaggle/input/lmsys-chatbot-arena\"\n",
    "TEST_CSV  = f\"{COMP_DIR}/test.csv\"\n",
    "\n",
    "# Example: if your dataset slug is yourname/lmsys-roberta-weights\n",
    "# and the folder inside is export_model/, then the path is:\n",
    "MODEL_DIR = \"/kaggle/input/lmsys-distillroberta/distilroberta-base_finetuned/checkpoint-6464\"   # <<< EDIT THIS\n",
    "\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 512\n",
    "BATCH   = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4ded0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:17.285610Z",
     "iopub.status.busy": "2025-11-30T21:02:17.285248Z",
     "iopub.status.idle": "2025-11-30T21:02:17.297654Z",
     "shell.execute_reply": "2025-11-30T21:02:17.296971Z"
    },
    "papermill": {
     "duration": 0.016434,
     "end_time": "2025-11-30T21:02:17.298820",
     "exception": false,
     "start_time": "2025-11-30T21:02:17.282386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def _strip_surrogates(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # replace invalid code points to keep kernel output stable\n",
    "    return text.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "\n",
    "def _strip_control(s: str) -> str:\n",
    "    # keep \\n and \\t; drop others\n",
    "    return re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \"\", s)\n",
    "\n",
    "def _safe_literal_list(text: str):\n",
    "    if isinstance(text, str) and text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(text)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def clean_text(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and (np.isnan(x) or np.isinf(x)):\n",
    "        return \"\"\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join(_strip_surrogates(_strip_control(str(t))) for t in x)\n",
    "    s = str(x).strip()\n",
    "    parsed = _safe_literal_list(s)\n",
    "    if parsed is not None:\n",
    "        return \" \".join(_strip_surrogates(_strip_control(str(t))) for t in parsed)\n",
    "    s = _strip_surrogates(s)\n",
    "    s = _strip_control(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73c6adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:17.303570Z",
     "iopub.status.busy": "2025-11-30T21:02:17.303181Z",
     "iopub.status.idle": "2025-11-30T21:02:17.310349Z",
     "shell.execute_reply": "2025-11-30T21:02:17.309652Z"
    },
    "papermill": {
     "duration": 0.010729,
     "end_time": "2025-11-30T21:02:17.311436",
     "exception": false,
     "start_time": "2025-11-30T21:02:17.300707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LMSYSDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int):\n",
    "        # Clean text once; DO NOT drop rows in code submission (must predict all ids)\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        for c in (\"prompt\", \"response_a\", \"response_b\"):\n",
    "            self.df[c] = self.df[c].apply(clean_text)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        text = f\"{r['prompt']} [SEP] {r['response_a']} [SEP] {r['response_b']}\"\n",
    "        enc = self.tok(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"id\": int(r[\"id\"]),\n",
    "        }\n",
    "\n",
    "def predict_proba(df, tokenizer, model, batch_size=BATCH, device=DEVICE):\n",
    "    ds = LMSYSDataset(df, tokenizer, MAX_LEN)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    ids_all, probs_all = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=attn).logits\n",
    "            probs = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "            ids_all.extend(batch[\"id\"].tolist())\n",
    "    if not probs_all:\n",
    "        return np.zeros((0,3)), []\n",
    "    return np.vstack(probs_all), ids_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce37e3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:17.315968Z",
     "iopub.status.busy": "2025-11-30T21:02:17.315731Z",
     "iopub.status.idle": "2025-11-30T21:02:17.320476Z",
     "shell.execute_reply": "2025-11-30T21:02:17.319808Z"
    },
    "papermill": {
     "duration": 0.008234,
     "end_time": "2025-11-30T21:02:17.321557",
     "exception": false,
     "start_time": "2025-11-30T21:02:17.313323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43df355d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:17.326494Z",
     "iopub.status.busy": "2025-11-30T21:02:17.326148Z",
     "iopub.status.idle": "2025-11-30T21:02:39.970939Z",
     "shell.execute_reply": "2025-11-30T21:02:39.970107Z"
    },
    "papermill": {
     "duration": 22.648716,
     "end_time": "2025-11-30T21:02:39.972252",
     "exception": false,
     "start_time": "2025-11-30T21:02:17.323536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:02:20.760927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764536540.931793      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764536540.982254      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test CSV (Kaggle replaces this with full hidden test during scoring)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Load local (attached) model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=3)\n",
    "if model.config.pad_token_id is None and tokenizer.pad_token_id is not None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c3ad38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:39.978593Z",
     "iopub.status.busy": "2025-11-30T21:02:39.978076Z",
     "iopub.status.idle": "2025-11-30T21:02:39.998677Z",
     "shell.execute_reply": "2025-11-30T21:02:39.998106Z"
    },
    "papermill": {
     "duration": 0.024822,
     "end_time": "2025-11-30T21:02:39.999754",
     "exception": false,
     "start_time": "2025-11-30T21:02:39.974932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
       "2  [\"To initialize the classification head when p...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e76b27b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T21:02:40.005646Z",
     "iopub.status.busy": "2025-11-30T21:02:40.005335Z",
     "iopub.status.idle": "2025-11-30T21:02:40.491620Z",
     "shell.execute_reply": "2025-11-30T21:02:40.490916Z"
    },
    "papermill": {
     "duration": 0.490574,
     "end_time": "2025-11-30T21:02:40.492866",
     "exception": false,
     "start_time": "2025-11-30T21:02:40.002292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /kaggle/working/submission.csv with 3 rows\n"
     ]
    }
   ],
   "source": [
    "# TTA 1: normal order (prompt, A, B)\n",
    "p_norm, ids_norm = predict_proba(test_df, tokenizer, model, BATCH, DEVICE)\n",
    "\n",
    "# TTA 2: swap A/B\n",
    "test_sw = test_df.copy()\n",
    "test_sw[[\"response_a\",\"response_b\"]] = test_sw[[\"response_b\",\"response_a\"]]\n",
    "p_swap, ids_swap = predict_proba(test_sw, tokenizer, model, BATCH, DEVICE)\n",
    "\n",
    "# Align ids just in case (should match)\n",
    "if ids_norm != ids_swap:\n",
    "    idx = pd.Series(np.arange(len(ids_swap)), index=ids_swap)\n",
    "    p_swap = p_swap[idx.loc[ids_norm].values]\n",
    "\n",
    "# Fix swapped class meaning: class0=first wins(B), class1=second wins(A)\n",
    "p_swap_fix = np.zeros_like(p_swap)\n",
    "p_swap_fix[:,0] = p_swap[:,1]  # B->A\n",
    "p_swap_fix[:,1] = p_swap[:,0]  # A->B\n",
    "p_swap_fix[:,2] = p_swap[:,2]  # tie\n",
    "\n",
    "final = (p_norm + p_swap_fix) / 2.0\n",
    "\n",
    "# Build submission\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": ids_norm,\n",
    "    \"winner_model_a\": final[:,0],\n",
    "    \"winner_model_b\": final[:,1],\n",
    "    \"winner_tie\":     final[:,2],\n",
    "})\n",
    "\n",
    "# Sanity checks\n",
    "assert sub.columns.tolist() == [\"id\",\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]\n",
    "assert np.isfinite(sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values).all()\n",
    "\n",
    "out_path = \"/kaggle/working/submission.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {out_path} with {len(sub)} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 8881873,
     "sourceId": 13936843,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8881831,
     "sourceId": 13936786,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.910994,
   "end_time": "2025-11-30T21:02:43.766441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-30T21:01:59.855447",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
